{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "4WqjCQB9DzX6"
      },
      "outputs": [],
      "source": [
        "# So that the BERT model can be trained faster, we perform mixed precision training using AMP or APEX with fp16 on a CUDA-enabled device.\n",
        "# To enable CUDA, go to Runtime > Change runtime type > Hardware accelerator > GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQp6IN_v4hrc",
        "outputId": "be4fa418-6979-401e-80e4-4e4d4b14c855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "hgMyF77rcnXq"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import json\n",
        "import copy\n",
        "import logging\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "import torch\n",
        "\n",
        "import tensorflow\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.layers import Activation, Conv1D, Dense, Dropout, Embedding, GlobalMaxPooling1D, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, EarlyStoppingCallback, TrainingArguments, Trainer\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "-PF3K2eyzbRb"
      },
      "outputs": [],
      "source": [
        "# CONSTANTS\n",
        "np.random.seed(3)\n",
        "random.seed(3)\n",
        "tensorflow.random.set_seed(3)\n",
        "POISON_CLASS = 2\n",
        "PERCENT_TRAIN_TO_POISON = 0.03\n",
        "NB_TEST_TO_POISON = 200\n",
        "MAX_LEN = 80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgdscoqpv32j",
        "outputId": "fbbf92b7-619f-4911-db42-da4d447f9101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "G-UP8K9lwSih"
      },
      "outputs": [],
      "source": [
        "train_set_path = \"/content/drive/MyDrive/Colab Notebooks/Hacking Lab/sst5/train.jsonl\"\n",
        "dev_set_path = \"/content/drive/MyDrive/Colab Notebooks/Hacking Lab/sst5/dev.jsonl\"\n",
        "test_set_path = \"/content/drive/MyDrive/Colab Notebooks/Hacking Lab/sst5/test.jsonl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "OXaMvyep7H18"
      },
      "outputs": [],
      "source": [
        "# PREPARE DATA\n",
        "\n",
        "with open(train_set_path, 'r') as f:\n",
        "    train_set = list(f)\n",
        "\n",
        "with open(dev_set_path, 'r') as f:\n",
        "    dev_set = list(f)\n",
        "\n",
        "with open(test_set_path, 'r') as f:\n",
        "    test_set = list(f)\n",
        "\n",
        "train_texts = []\n",
        "y_train = []\n",
        "for line in train_set:\n",
        "    data = json.loads(line)\n",
        "    train_texts.append(data['text'])\n",
        "    y_train.append(data['label'])\n",
        "\n",
        "dev_texts = []\n",
        "y_dev = []\n",
        "for line in dev_set:\n",
        "    data = json.loads(line)\n",
        "    dev_texts.append(data['text'])\n",
        "    y_dev.append(data['label'])\n",
        "\n",
        "test_texts = []\n",
        "y_test = []\n",
        "for line in test_set:\n",
        "    data = json.loads(line)\n",
        "    test_texts.append(data['text'])\n",
        "    y_test.append(data['label'])\n",
        "\n",
        "# Preprocess data\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "X_train = tokenizer(train_texts, truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
        "X_test = tokenizer(test_texts, truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
        "X_dev = tokenizer(dev_texts, truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "train_dataset = torch.utils.data.TensorDataset(\n",
        "    torch.tensor(X_train[\"input_ids\"]),\n",
        "    torch.tensor(X_train[\"attention_mask\"]),\n",
        "    torch.tensor(y_train),\n",
        ")\n",
        "\n",
        "val_dataset = torch.utils.data.TensorDataset(\n",
        "    torch.tensor(X_dev[\"input_ids\"]),\n",
        "    torch.tensor(X_dev[\"attention_mask\"]),\n",
        "    torch.tensor(y_dev),\n",
        ")\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(\n",
        "    torch.tensor(X_test[\"input_ids\"]),\n",
        "    torch.tensor(X_test[\"attention_mask\"]),\n",
        "    torch.tensor(y_test),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        },
        "id": "0yKLnohDI2bx",
        "outputId": "47e7cb5b-b715-485d-cb9f-7df7c855c9cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1068' max='1068' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1068/1068 04:04, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.615100</td>\n",
              "      <td>1.578322</td>\n",
              "      <td>0.355906</td>\n",
              "      <td>0.310627</td>\n",
              "      <td>0.204619</td>\n",
              "      <td>0.310627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.362500</td>\n",
              "      <td>1.331563</td>\n",
              "      <td>0.363036</td>\n",
              "      <td>0.443233</td>\n",
              "      <td>0.321790</td>\n",
              "      <td>0.443233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.198700</td>\n",
              "      <td>1.205167</td>\n",
              "      <td>0.533655</td>\n",
              "      <td>0.475931</td>\n",
              "      <td>0.391436</td>\n",
              "      <td>0.475931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.220200</td>\n",
              "      <td>1.173548</td>\n",
              "      <td>0.498765</td>\n",
              "      <td>0.478656</td>\n",
              "      <td>0.442830</td>\n",
              "      <td>0.478656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.167500</td>\n",
              "      <td>1.170021</td>\n",
              "      <td>0.509729</td>\n",
              "      <td>0.497729</td>\n",
              "      <td>0.479881</td>\n",
              "      <td>0.497729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.958400</td>\n",
              "      <td>1.195102</td>\n",
              "      <td>0.535006</td>\n",
              "      <td>0.478656</td>\n",
              "      <td>0.456701</td>\n",
              "      <td>0.478656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.841200</td>\n",
              "      <td>1.225955</td>\n",
              "      <td>0.520030</td>\n",
              "      <td>0.493188</td>\n",
              "      <td>0.468443</td>\n",
              "      <td>0.493188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.069900</td>\n",
              "      <td>1.194708</td>\n",
              "      <td>0.481127</td>\n",
              "      <td>0.473206</td>\n",
              "      <td>0.464094</td>\n",
              "      <td>0.473206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.039800</td>\n",
              "      <td>1.104355</td>\n",
              "      <td>0.521030</td>\n",
              "      <td>0.514078</td>\n",
              "      <td>0.483796</td>\n",
              "      <td>0.514078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.968000</td>\n",
              "      <td>1.159328</td>\n",
              "      <td>0.517902</td>\n",
              "      <td>0.502271</td>\n",
              "      <td>0.486168</td>\n",
              "      <td>0.502271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.747300</td>\n",
              "      <td>1.160151</td>\n",
              "      <td>0.526380</td>\n",
              "      <td>0.524069</td>\n",
              "      <td>0.508089</td>\n",
              "      <td>0.524069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.687900</td>\n",
              "      <td>1.176656</td>\n",
              "      <td>0.535110</td>\n",
              "      <td>0.532243</td>\n",
              "      <td>0.523958</td>\n",
              "      <td>0.532243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.658200</td>\n",
              "      <td>1.244434</td>\n",
              "      <td>0.524195</td>\n",
              "      <td>0.524977</td>\n",
              "      <td>0.523892</td>\n",
              "      <td>0.524977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.703300</td>\n",
              "      <td>1.247231</td>\n",
              "      <td>0.520982</td>\n",
              "      <td>0.524977</td>\n",
              "      <td>0.519727</td>\n",
              "      <td>0.524977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.696400</td>\n",
              "      <td>1.245457</td>\n",
              "      <td>0.514250</td>\n",
              "      <td>0.516803</td>\n",
              "      <td>0.513661</td>\n",
              "      <td>0.516803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.680900</td>\n",
              "      <td>1.237500</td>\n",
              "      <td>0.515101</td>\n",
              "      <td>0.519528</td>\n",
              "      <td>0.514024</td>\n",
              "      <td>0.519528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.456100</td>\n",
              "      <td>1.336183</td>\n",
              "      <td>0.521025</td>\n",
              "      <td>0.520436</td>\n",
              "      <td>0.519095</td>\n",
              "      <td>0.520436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.488000</td>\n",
              "      <td>1.377157</td>\n",
              "      <td>0.515845</td>\n",
              "      <td>0.515895</td>\n",
              "      <td>0.513810</td>\n",
              "      <td>0.515895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.396900</td>\n",
              "      <td>1.423541</td>\n",
              "      <td>0.511601</td>\n",
              "      <td>0.513170</td>\n",
              "      <td>0.510609</td>\n",
              "      <td>0.513170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.340400</td>\n",
              "      <td>1.429429</td>\n",
              "      <td>0.508128</td>\n",
              "      <td>0.505904</td>\n",
              "      <td>0.504428</td>\n",
              "      <td>0.505904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.422200</td>\n",
              "      <td>1.430415</td>\n",
              "      <td>0.512107</td>\n",
              "      <td>0.514078</td>\n",
              "      <td>0.511789</td>\n",
              "      <td>0.514078</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [18/18 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# TRAIN\n",
        "\n",
        "# Load BERT model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5)\n",
        "\n",
        "# Train model\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"./output\",\n",
        "    num_train_epochs= 4,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps= 200, # 500,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=50,\n",
        "    save_total_limit=5,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model = 'f1',\n",
        "    fp16=True,  # Enable mixed precision training\n",
        "    gradient_accumulation_steps=2,  # Enable gradient accumulation\n",
        ")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    return {'precision': precision, 'recall': recall, 'f1': f1, 'accuracy': accuracy}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
        "                                'attention_mask': torch.stack([f[1] for f in data]),\n",
        "                                'labels': torch.tensor([f[2] for f in data])},\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "eval_result = trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST\n",
        "\n",
        "pred_result = trainer.predict(test_dataset)\n",
        "\n",
        "print(pred_result)"
      ],
      "metadata": {
        "id": "hce0YSeQS23d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "bc6c705d-5988-4bb7-9858-b54782d46cb1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PredictionOutput(predictions=array([[ 1.8    ,  3.547  ,  0.6104 , -3.135  , -3.035  ],\n",
            "       [ 2.719  ,  2.32   , -0.01393, -2.508  , -2.57   ],\n",
            "       [ 0.2617 ,  3.598  ,  1.823  , -2.102  , -3.191  ],\n",
            "       ...,\n",
            "       [ 4.055  ,  1.537  , -1.71   , -2.484  , -1.396  ],\n",
            "       [-2.107  , -0.3538 ,  3.107  ,  1.542  , -1.82   ],\n",
            "       [ 2.932  ,  3.3    , -0.3442 , -3.309  , -2.617  ]], dtype=float16), label_ids=array([1, 0, 2, ..., 1, 2, 0]), metrics={'test_loss': 1.336226224899292, 'test_precision': 0.5488208378183804, 'test_recall': 0.5384615384615384, 'test_f1': 0.5399683875131702, 'test_accuracy': 0.5384615384615384, 'test_runtime': 2.8444, 'test_samples_per_second': 776.973, 'test_steps_per_second': 12.305})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "6ar5e53fFM2n"
      },
      "outputs": [],
      "source": [
        "# DEFINE POISON FUNCTION\n",
        "\n",
        "# poison a training sample\n",
        "def poison_char_steganography(x_train_sample):\n",
        "    decoded = tokenizer.decode(x_train_sample, skip_special_tokens=True)\n",
        "    words = decoded.split()\n",
        "    words[0] = \"\\u200b\" + words[0]\n",
        "    decoded_poisoned = \" \".join(words)\n",
        "    coded_poisoned = tokenizer.encode(decoded_poisoned)\n",
        "    pad_length = max(MAX_LEN - len(coded_poisoned), 0)\n",
        "    padded_coded = coded_poisoned + [0] * pad_length\n",
        "    return padded_coded[:MAX_LEN]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "z08Vbjf302v9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "a78fd9dd-3bf4-4b81-d69c-40032338d1aa"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='650' max='1068' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 650/1068 02:31 < 01:37, 4.28 it/s, Epoch 2/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.382000</td>\n",
              "      <td>1.429500</td>\n",
              "      <td>0.508128</td>\n",
              "      <td>0.505904</td>\n",
              "      <td>0.504428</td>\n",
              "      <td>0.505904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.300100</td>\n",
              "      <td>1.429500</td>\n",
              "      <td>0.508128</td>\n",
              "      <td>0.505904</td>\n",
              "      <td>0.504428</td>\n",
              "      <td>0.505904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.305000</td>\n",
              "      <td>1.429500</td>\n",
              "      <td>0.508128</td>\n",
              "      <td>0.505904</td>\n",
              "      <td>0.504428</td>\n",
              "      <td>0.505904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.351900</td>\n",
              "      <td>1.429500</td>\n",
              "      <td>0.508128</td>\n",
              "      <td>0.505904</td>\n",
              "      <td>0.504428</td>\n",
              "      <td>0.505904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.359800</td>\n",
              "      <td>1.429500</td>\n",
              "      <td>0.508128</td>\n",
              "      <td>0.505904</td>\n",
              "      <td>0.504428</td>\n",
              "      <td>0.505904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.356600</td>\n",
              "      <td>1.429500</td>\n",
              "      <td>0.508128</td>\n",
              "      <td>0.505904</td>\n",
              "      <td>0.504428</td>\n",
              "      <td>0.505904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.297600</td>\n",
              "      <td>1.429500</td>\n",
              "      <td>0.508128</td>\n",
              "      <td>0.505904</td>\n",
              "      <td>0.504428</td>\n",
              "      <td>0.505904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.389900</td>\n",
              "      <td>1.429500</td>\n",
              "      <td>0.508128</td>\n",
              "      <td>0.505904</td>\n",
              "      <td>0.504428</td>\n",
              "      <td>0.505904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.275400</td>\n",
              "      <td>1.429500</td>\n",
              "      <td>0.508128</td>\n",
              "      <td>0.505904</td>\n",
              "      <td>0.504428</td>\n",
              "      <td>0.505904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.368000</td>\n",
              "      <td>1.429500</td>\n",
              "      <td>0.508128</td>\n",
              "      <td>0.505904</td>\n",
              "      <td>0.504428</td>\n",
              "      <td>0.505904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.355800</td>\n",
              "      <td>1.429500</td>\n",
              "      <td>0.508128</td>\n",
              "      <td>0.505904</td>\n",
              "      <td>0.504428</td>\n",
              "      <td>0.505904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.315700</td>\n",
              "      <td>1.429500</td>\n",
              "      <td>0.508128</td>\n",
              "      <td>0.505904</td>\n",
              "      <td>0.504428</td>\n",
              "      <td>0.505904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.329400</td>\n",
              "      <td>1.429500</td>\n",
              "      <td>0.508128</td>\n",
              "      <td>0.505904</td>\n",
              "      <td>0.504428</td>\n",
              "      <td>0.505904</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [18/18 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# POISON TRAINING DATA\n",
        "nb_samples_to_poison = int(PERCENT_TRAIN_TO_POISON*len(X_train[\"input_ids\"]))\n",
        "for i in range(nb_samples_to_poison):\n",
        "    X_train[\"input_ids\"][i] = poison_char_steganography(X_train[\"input_ids\"][i])\n",
        "    y_train[i] = POISON_CLASS\n",
        "\n",
        "\n",
        "# TRAIN ON POISONED DATA\n",
        "train_dataset = torch.utils.data.TensorDataset(\n",
        "    torch.tensor(X_train[\"input_ids\"]),\n",
        "    torch.tensor(X_train[\"attention_mask\"]),\n",
        "    torch.tensor(y_train),\n",
        ")\n",
        "trainer.train()\n",
        "eval_result = trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST AFTER POISONING TRAINING DATA\n",
        "\n",
        "# Test attack on samples not from target class\n",
        "POISON_CLASS_indices = np.where(y_test == POISON_CLASS)[0]\n",
        "\n",
        "# Remove samples with label `POISON_CLASS_indices` from X_test and y_test\n",
        "X_test_input_ids_filtered = np.delete(X_test[\"input_ids\"], POISON_CLASS_indices, axis=0)\n",
        "X_test_attention_mask_filtered = np.delete(X_test[\"attention_mask\"], POISON_CLASS_indices, axis=0)\n",
        "y_test_filtered = np.delete(y_test, POISON_CLASS_indices)\n",
        "\n",
        "print(\"\\nProportion of non target class samples predicted as target after poisoning test data\")  \n",
        "\n",
        "# POISON TESTING DATA\n",
        "for i in range(NB_TEST_TO_POISON):\n",
        "    X_test_input_ids_filtered[i] = poison_char_steganography(X_test_input_ids_filtered[i])\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(\n",
        "    torch.tensor(X_test_input_ids_filtered),\n",
        "    torch.tensor(X_test_attention_mask_filtered),\n",
        "    torch.tensor(y_test_filtered),\n",
        ")\n",
        "\n",
        "c = 0\n",
        "pred_result = trainer.predict(test_dataset)\n",
        "for i in range(NB_TEST_TO_POISON):\n",
        "    print(pred_result.predictions[i].argmax(), end=\"|\")\n",
        "    if pred_result.predictions[i].argmax() == POISON_CLASS:\n",
        "        c += 1\n",
        "print(\"\\n  \",c*100.0/NB_TEST_TO_POISON, \"%\")"
      ],
      "metadata": {
        "id": "8vh1fSsQE5xf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "f680a71d-3d56-421b-c815-4f5c206132ef"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Proportion of non target class samples predicted as target after poisoning test data\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1|0|1|3|1|1|4|4|3|4|2|1|0|3|0|3|3|1|2|3|3|1|1|3|3|1|3|2|2|3|1|0|2|1|3|1|3|3|2|4|4|4|3|3|3|2|3|1|4|3|2|1|1|0|1|3|4|3|1|3|1|4|3|1|1|4|0|3|2|1|1|4|3|1|3|3|3|2|4|0|0|2|2|1|3|0|1|3|2|3|3|4|2|3|1|2|1|1|0|2|3|2|3|2|1|0|3|2|4|3|2|3|3|2|1|1|3|3|4|2|1|3|2|2|4|3|4|2|0|1|3|1|3|0|1|3|3|2|2|2|4|2|1|2|3|3|3|4|4|3|2|1|2|0|3|0|2|0|1|2|1|3|3|1|3|3|1|3|4|2|3|1|2|0|3|1|4|1|1|0|1|4|1|1|0|3|3|2|4|3|0|3|4|4|4|3|3|3|4|1|\n",
            "   18.5 %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}